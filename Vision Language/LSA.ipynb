{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras import Input, Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer, Embedding, LayerNormalization, MultiHeadAttention, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Input, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Multiply, Dense, Layer, Dropout, LayerNormalization\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/MultiModal/memotion_dataset_7k/labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.filter(regex='unnamed', axis=1), inplace=True, axis=1)\n",
    "df.drop(columns=['text_ocr', 'humour', \"motivational\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_replace = {'positive': 0, 'neutral': 1, 'very_positive': 2, 'negative': 3, 'very_negative': 4}\n",
    "df['overall_sentiment'] = df['overall_sentiment'].replace(set_replace)\n",
    "cat_replace = {'not_offensive': 0, 'slight': 1, 'very_offensive': 2, 'hateful_offensive': 3}\n",
    "df['offensive'] = df['offensive'].replace(cat_replace)\n",
    "sar_replace = {'general': 0, 'twisted_meaning': 1, 'not_sarcastic': 2, 'very_twisted': 3}\n",
    "df['sarcasm'] = df['sarcasm'].replace(sar_replace)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "rows_to_drop = ['image_120.jpg', 'image_4800.jpg', 'image_6782.jpg', 'image_6785.jpg', \n",
    "                'image_6787.jpg', 'image_6988.jpg', 'image_6989.jpg', 'image_6990.png',\n",
    "                'image_6991.jpg', 'image_6992.jpg']\n",
    "for image_name in rows_to_drop:\n",
    "    df.drop(df[df['image_name'] == image_name].index, inplace=True)\n",
    "\n",
    "width, height = 120, 120\n",
    "image_folder = \"E:/MultiModal/memotion_dataset_7k/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned_text'] = df['text_corrected'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['cleaned_text', 'offensive', 'overall_sentiment', 'sarcasm', 'image_name']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentiment_size = df['overall_sentiment'].value_counts().max()\n",
    "max_offensive_size = df['offensive'].value_counts().max()  \n",
    "max_sarcasm_size = df['sarcasm'].value_counts().max()  \n",
    "target_size = max(max_sentiment_size, max_offensive_size, max_sarcasm_size) \n",
    "\n",
    "def upsample_classes(df, target_column, target_size):\n",
    "    classes = df[target_column].unique()\n",
    "    resampled_list = []\n",
    "    \n",
    "    for class_value in classes:\n",
    "        class_data = df[df[target_column] == class_value]\n",
    "        resampled_class = resample(class_data,\n",
    "                                   replace=True,  \n",
    "                                   n_samples=target_size,\n",
    "                                   random_state=42)\n",
    "        resampled_list.append(resampled_class)\n",
    "    \n",
    "    return pd.concat(resampled_list)\n",
    "\n",
    "df = upsample_classes(df, 'overall_sentiment', target_size)\n",
    "df = upsample_classes(df, 'offensive', target_size)\n",
    "df = upsample_classes(df, 'sarcasm', target_size)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df['overall_sentiment'].value_counts())\n",
    "print(df['offensive'].value_counts())\n",
    "print(df['sarcasm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for image_name in tqdm(df['image_name']):\n",
    "    path = os.path.join(image_folder, image_name)\n",
    "    try:\n",
    "        img = Image.open(path).convert('RGB').resize((width, height))\n",
    "        img = np.array(img) / 255.0 \n",
    "        X.append(img)\n",
    "    except (OSError, IOError) as e:\n",
    "        print(f\"Error loading image {image_name}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "print(f\"Image data shape: {X.shape}\")\n",
    "y = df[['offensive', 'overall_sentiment', \"sarcasm\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 60\n",
    "text_data = df['cleaned_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['cleaned_text'].values\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in tqdm(text_data):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    input_ids.append(encoded['input_ids'])\n",
    "    attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Masks shape: {attention_masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "\n",
    "input_ids = np.squeeze(input_ids, axis=1)\n",
    "attention_masks = np.squeeze(attention_masks, axis=1)\n",
    "print(f\"Input IDs shape after squeezing: {input_ids.shape}\")\n",
    "print(f\"Attention Masks shape after squeezing: {attention_masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, train_masks, test_masks = train_test_split(\n",
    "    input_ids, attention_masks, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sentiment = y_train['overall_sentiment']\n",
    "y_train_offensive = y_train['offensive']\n",
    "y_train_sarcasm = y_train['sarcasm']\n",
    "\n",
    "y_test_sentiment = y_test['overall_sentiment']\n",
    "y_test_offensive = y_test['offensive']\n",
    "y_test_sarcasm = y_test['sarcasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    elif epoch >= 5 and epoch < 11:\n",
    "        return lr * 0.5 \n",
    "    else:\n",
    "        return lr * 0.1 \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        \n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  \n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) \n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_len\": self.max_len,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = tf.expand_dims(mask, axis=1) \n",
    "            mask = tf.expand_dims(mask, axis=1) \n",
    "\n",
    "        attn_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(vocab_size, max_len, embed_dim, num_heads, ff_dim, num_layers):\n",
    "    text_input = Input(shape=(max_len,), dtype=tf.int32, name='text_input')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(text_input)\n",
    "    positional_encoding_layer = PositionalEncoding(max_len, embed_dim)(embedding_layer)\n",
    "    \n",
    "    x = positional_encoding_layer\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x, mask=attention_mask)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    \n",
    "    return Model(inputs=[text_input, attention_mask], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        patch_size = input_shape[1] // int(np.sqrt(self.num_patches))\n",
    "        self.flatten_patches = tf.keras.layers.Reshape(target_shape=(-1, patch_size * patch_size * input_shape[-1]))\n",
    "        self.dense_projection = Dense(self.projection_dim)\n",
    "\n",
    "    def call(self, images):\n",
    "        patches = self.flatten_patches(images)\n",
    "        projected_patches = self.dense_projection(patches)\n",
    "        return projected_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, num_heads, ff_dim, num_layers, residual=True, use_glu=True):\n",
    "        super(ViT, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.residual = residual\n",
    "        self.use_glu = use_glu\n",
    "\n",
    "        # Patch and positional embeddings\n",
    "        self.patch_embedding = PatchEmbedding(num_patches, projection_dim)\n",
    "        self.position_embedding = PositionalEncoding(num_patches, projection_dim)\n",
    "\n",
    "        # MultiHeadAttention and FeedForward layers for each Transformer layer\n",
    "        self.attention_layers = [\n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                num_heads=num_heads + i,\n",
    "                key_dim=projection_dim,\n",
    "                attention_axes=(1,)  # Attention only across the patch dimension\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Initialize Dense layers for GLU outside the call method\n",
    "        if self.use_glu:\n",
    "            self.glu_dense_linear = [Dense(ff_dim) for _ in range(num_layers)]\n",
    "            self.glu_dense_gate = [Dense(ff_dim, activation='sigmoid') for _ in range(num_layers)]\n",
    "            self.glu_output_proj = [Dense(projection_dim) for _ in range(num_layers)]\n",
    "        else:\n",
    "            self.ffn_layers = [\n",
    "                self.build_ffn_layer(projection_dim, ff_dim)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Layer norms for pre-attention and pre-FFN normalization\n",
    "        self.pre_layernorm_layers = [\n",
    "            LayerNormalization(epsilon=1e-6)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.post_layernorm_layers = [\n",
    "            LayerNormalization(epsilon=1e-6)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Adding multiple Dropout layers for regularization\n",
    "        self.dropout_layers = [\n",
    "            Dropout(0.1 + 0.05 * i)\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def build_ffn_layer(self, projection_dim, ff_dim):\n",
    "        # Define the standard FFN layer outside the call method\n",
    "        return tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(projection_dim)\n",
    "        ])\n",
    "\n",
    "    def gated_ffn(self, inputs, layer_idx):\n",
    "        # Gated Linear Units (GLU) implementation\n",
    "        linear_output = self.glu_dense_linear[layer_idx](inputs)\n",
    "        gate = self.glu_dense_gate[layer_idx](inputs)\n",
    "        gated_output = Multiply()([linear_output, gate])\n",
    "        return self.glu_output_proj[layer_idx](gated_output)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initial embedding and position encoding\n",
    "        x = self.patch_embedding(inputs)\n",
    "        x = self.position_embedding(x)\n",
    "\n",
    "        # Transformer layers with multi-head attention and FFN\n",
    "        for i in range(self.num_layers):\n",
    "            # Pre-Attention LayerNorm\n",
    "            norm_x = self.pre_layernorm_layers[i](x)\n",
    "\n",
    "            # Multi-Head Attention with residual connection\n",
    "            attn_output = self.attention_layers[i](norm_x, norm_x)\n",
    "            attn_output = self.dropout_layers[i](attn_output)\n",
    "            if self.residual:\n",
    "                x = x + attn_output  # Residual connection\n",
    "\n",
    "            # Pre-FFN LayerNorm\n",
    "            norm_x = self.post_layernorm_layers[i](x)\n",
    "\n",
    "            # Feedforward network with possible GLU and residual\n",
    "            if self.use_glu:\n",
    "                ffn_output = self.gated_ffn(norm_x, i)\n",
    "            else:\n",
    "                ffn_output = self.ffn_layers[i](norm_x)\n",
    "\n",
    "            ffn_output = self.dropout_layers[i](ffn_output)\n",
    "            if self.residual:\n",
    "                x = x + ffn_output  # Residual connection\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 120\n",
    "patch_size = 10   \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 128 \n",
    "\n",
    "vocab_size = tokenizer.vocab_size + 1\n",
    "embed_dim = 128\n",
    "num_heads = 2\n",
    "ff_dim = 512\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttentiveFusion(Layer):\n",
    "    def __init__(self, projection_dim, **kwargs):\n",
    "        super(LinearSelfAttentiveFusion, self).__init__(**kwargs)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.W_q = Dense(projection_dim)  \n",
    "        self.W_k = Dense(projection_dim)\n",
    "        self.W_v = Dense(projection_dim) \n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def call(self, image_features, text_features):\n",
    "        Q = self.W_q(image_features)\n",
    "        K = self.W_k(text_features)\n",
    "        V = self.W_v(text_features)\n",
    "\n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True)\n",
    "        attention_scores /= tf.sqrt(tf.cast(self.projection_dim, tf.float32))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        fusion_output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        return fusion_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 120\n",
    "patch_size = 10   \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 128\n",
    "vocab_size = tokenizer.vocab_size + 1\n",
    "embed_dim = 128\n",
    "num_heads = 2\n",
    "ff_dim = 512\n",
    "num_layers = 2\n",
    "max_len = 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape=(image_size, image_size, 3), name='image_input')\n",
    "x = ViT(num_patches, projection_dim, num_heads, ff_dim, num_layers)(image_input)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "text_input = Input(shape=(max_len,), dtype=tf.int32, name='text_input')\n",
    "attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "text_features = build_transformer_model(vocab_size, max_len, embed_dim, num_heads, ff_dim, num_layers)([text_input, attention_mask])\n",
    "\n",
    "combined = Concatenate()([x, text_features])\n",
    "\n",
    "fusion_layer = LinearSelfAttentiveFusion(projection_dim)(x, text_features)\n",
    "\n",
    "combined_fusion = Concatenate()([fusion_layer, combined])\n",
    "\n",
    "sentiment_output = Dense(5, activation='softmax', name='sentiment_output')(combined_fusion)\n",
    "offensive_output = Dense(4, activation='softmax', name='offensive_output')(combined_fusion)\n",
    "sarcasm_output = Dense(4, activation='softmax', name='sarcasm_output')(combined_fusion)\n",
    "\n",
    "model = Model(inputs=[image_input, text_input, attention_mask],\n",
    "              outputs=[offensive_output, sentiment_output, sarcasm_output])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss={'sentiment_output': 'sparse_categorical_crossentropy',\n",
    "                    'offensive_output': 'sparse_categorical_crossentropy',\n",
    "                    \"sarcasm_output\": \"sparse_categorical_crossentropy\"},\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [X_train, X_train_text, train_masks], \n",
    "    {'offensive_output': y_train_offensive, 'sentiment_output': y_train_sentiment, 'sarcasm_output': y_train_sarcasm},\n",
    "    validation_data=([X_test, X_test_text, test_masks], \n",
    "                     {'offensive_output': y_test_offensive, 'sentiment_output': y_test_sentiment, 'sarcasm_output': y_test_sarcasm}),\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history_dict['loss'], label='Training Loss')\n",
    "plt.plot(epochs, history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history_dict['offensive_output_accuracy'],  label='Training Offensive Accuracy')\n",
    "plt.plot(epochs, history_dict['val_offensive_output_accuracy'],  label='Validation Offensive Accuracy')\n",
    "plt.plot(epochs, history_dict['sentiment_output_accuracy'], label='Training Sentiment Accuracy')\n",
    "plt.plot(epochs, history_dict['val_sentiment_output_accuracy'],  label='Validation Sentiment Accuracy')\n",
    "plt.plot(epochs, history_dict['sarcasm_output_accuracy'], label='Training Sarcasm Accuracy')\n",
    "plt.plot(epochs, history_dict['val_sarcasm_output_accuracy'],  label='Validation Sarcasm Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "eval_results = model.evaluate(\n",
    "    [X_test, X_test_text, test_masks], \n",
    "    {\n",
    "        'offensive_output': y_test_offensive, \n",
    "        'sentiment_output': y_test_sentiment, \n",
    "        'sarcasm_output': y_test_sarcasm\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extracting loss and accuracy metrics from evaluation results\n",
    "test_loss = eval_results[0]  # Total loss\n",
    "offensive_output_loss = eval_results[1]  # Loss for offensive task\n",
    "sentiment_output_loss = eval_results[2]  # Loss for sentiment task\n",
    "sarcasm_output_loss = eval_results[3]    # Loss for sarcasm task\n",
    "\n",
    "offensive_output_accuracy = eval_results[4]  # Accuracy for offensive task\n",
    "sentiment_output_accuracy = eval_results[5]  # Accuracy for sentiment task\n",
    "sarcasm_output_accuracy = eval_results[6]    # Accuracy for sarcasm task\n",
    "\n",
    "# Printing results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Offensive Output Loss: {offensive_output_loss}\")\n",
    "print(f\"Sentiment Output Loss: {sentiment_output_loss}\")\n",
    "print(f\"Sarcasm Output Loss: {sarcasm_output_loss}\")\n",
    "\n",
    "print(f\"Offensive Output Accuracy: {offensive_output_accuracy}\")\n",
    "print(f\"Sentiment Output Accuracy: {sentiment_output_accuracy}\")\n",
    "print(f\"Sarcasm Output Accuracy: {sarcasm_output_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "y_pred = model.predict([X_test, X_test_text, test_masks])\n",
    "\n",
    "# Since the model has three outputs, get the predictions for each output\n",
    "y_pred_offensive = np.argmax(y_pred[0], axis=1)\n",
    "y_pred_sentiment = np.argmax(y_pred[1], axis=1)\n",
    "y_pred_sarcasm = np.argmax(y_pred[2], axis=1)\n",
    "\n",
    "# Print classification reports for each output\n",
    "print(\"\\n--- Offensive Output Classification Report ---\")\n",
    "print(classification_report(y_test_offensive, y_pred_offensive, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"]))\n",
    "\n",
    "print(\"\\n--- Sentiment Output Classification Report ---\")\n",
    "print(classification_report(y_test_sentiment, y_pred_sentiment, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]))\n",
    "\n",
    "print(\"\\n--- Sarcasm Output Classification Report ---\")\n",
    "print(classification_report(y_test_sarcasm, y_pred_sarcasm, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"]))\n",
    "\n",
    "# Macro metrics\n",
    "macro_precision_offensive = precision_score(y_test_offensive, y_pred_offensive, average='macro')\n",
    "macro_recall_offensive = recall_score(y_test_offensive, y_pred_offensive, average='macro')\n",
    "macro_f1_offensive = f1_score(y_test_offensive, y_pred_offensive, average='macro')\n",
    "\n",
    "macro_precision_sentiment = precision_score(y_test_sentiment, y_pred_sentiment, average='macro')\n",
    "macro_recall_sentiment = recall_score(y_test_sentiment, y_pred_sentiment, average='macro')\n",
    "macro_f1_sentiment = f1_score(y_test_sentiment, y_pred_sentiment, average='macro')\n",
    "\n",
    "macro_precision_sarcasm = precision_score(y_test_sarcasm, y_pred_sarcasm, average='macro')\n",
    "macro_recall_sarcasm = recall_score(y_test_sarcasm, y_pred_sarcasm, average='macro')\n",
    "macro_f1_sarcasm = f1_score(y_test_sarcasm, y_pred_sarcasm, average='macro')\n",
    "\n",
    "# Micro metrics\n",
    "micro_precision_offensive = precision_score(y_test_offensive, y_pred_offensive, average='micro')\n",
    "micro_recall_offensive = recall_score(y_test_offensive, y_pred_offensive, average='micro')\n",
    "micro_f1_offensive = f1_score(y_test_offensive, y_pred_offensive, average='micro')\n",
    "\n",
    "micro_precision_sentiment = precision_score(y_test_sentiment, y_pred_sentiment, average='micro')\n",
    "micro_recall_sentiment = recall_score(y_test_sentiment, y_pred_sentiment, average='micro')\n",
    "micro_f1_sentiment = f1_score(y_test_sentiment, y_pred_sentiment, average='micro')\n",
    "\n",
    "micro_precision_sarcasm = precision_score(y_test_sarcasm, y_pred_sarcasm, average='micro')\n",
    "micro_recall_sarcasm = recall_score(y_test_sarcasm, y_pred_sarcasm, average='micro')\n",
    "micro_f1_sarcasm = f1_score(y_test_sarcasm, y_pred_sarcasm, average='micro')\n",
    "\n",
    "# Print macro metrics\n",
    "print(\"\\n--- Offensive Output Macro Metrics ---\")\n",
    "print(f\"Macro Precision: {macro_precision_offensive}\")\n",
    "print(f\"Macro Recall: {macro_recall_offensive}\")\n",
    "print(f\"Macro F1: {macro_f1_offensive}\")\n",
    "\n",
    "print(\"\\n--- Sentiment Output Macro Metrics ---\")\n",
    "print(f\"Macro Precision: {macro_precision_sentiment}\")\n",
    "print(f\"Macro Recall: {macro_recall_sentiment}\")\n",
    "print(f\"Macro F1: {macro_f1_sentiment}\")\n",
    "\n",
    "print(\"\\n--- Sarcasm Output Macro Metrics ---\")\n",
    "print(f\"Macro Precision: {macro_precision_sarcasm}\")\n",
    "print(f\"Macro Recall: {macro_recall_sarcasm}\")\n",
    "print(f\"Macro F1: {macro_f1_sarcasm}\")\n",
    "\n",
    "# Print micro metrics\n",
    "print(\"\\n--- Offensive Output Micro Metrics ---\")\n",
    "print(f\"Micro Precision: {micro_precision_offensive}\")\n",
    "print(f\"Micro Recall: {micro_recall_offensive}\")\n",
    "print(f\"Micro F1: {micro_f1_offensive}\")\n",
    "\n",
    "print(\"\\n--- Sentiment Output Micro Metrics ---\")\n",
    "print(f\"Micro Precision: {micro_precision_sentiment}\")\n",
    "print(f\"Micro Recall: {micro_recall_sentiment}\")\n",
    "print(f\"Micro F1: {micro_f1_sentiment}\")\n",
    "\n",
    "print(\"\\n--- Sarcasm Output Micro Metrics ---\")\n",
    "print(f\"Micro Precision: {micro_precision_sarcasm}\")\n",
    "print(f\"Micro Recall: {micro_recall_sarcasm}\")\n",
    "print(f\"Micro F1: {micro_f1_sarcasm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_test, X_test_text, test_masks])\n",
    "\n",
    "y_pred_offensive = y_pred[0]\n",
    "y_pred_sentiment = y_pred[1]\n",
    "y_pred_sarcasm = y_pred[2]\n",
    "\n",
    "y_pred_offensive_classes = np.argmax(y_pred_offensive, axis=-1)\n",
    "y_pred_sentiment_classes = np.argmax(y_pred_sentiment, axis=-1)\n",
    "y_pred_sarcasm_classes = np.argmax(y_pred_sarcasm, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_offensive = confusion_matrix(y_test_offensive, y_pred_offensive_classes)\n",
    "cm_sentiment = confusion_matrix(y_test_sentiment, y_pred_sentiment_classes)\n",
    "cm_sarcasm = confusion_matrix(y_test_sarcasm, y_pred_sarcasm_classes)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "sns.heatmap(cm_offensive, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['not_offensive', 'slight', 'very_offensive', 'hateful_offensive'], \n",
    "            yticklabels=['not_offensive', 'slight', 'very_offensive', 'hateful_offensive'])\n",
    "plt.title('Confusion Matrix - Offensive')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "sns.heatmap(cm_sentiment, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['positive', 'neutral', 'very_positive', 'negative', 'very_negative'], \n",
    "            yticklabels=['positive', 'neutral', 'very_positive', 'negative', 'very_negative'])\n",
    "plt.title('Confusion Matrix - Sentiment')\n",
    "sar_replace = {'general': 0, 'twisted_meaning': 1, 'not_sarcastic': 2, 'very_twisted': 3}\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "\n",
    "sns.heatmap(cm_sarcasm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['general', 'twisted_meaning', 'not_sarcastic', 'very_twisted'], \n",
    "            yticklabels=['general', 'twisted_meaning', 'not_sarcastic', 'very_twisted'])\n",
    "plt.title('Sarcasm Matrix - Scarcasm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_offensive_bin = label_binarize(y_test_offensive, classes=[0, 1, 2, 3])\n",
    "y_test_sentiment_bin = label_binarize(y_test_sentiment, classes=[0, 1, 2, 3, 4])\n",
    "y_test_sarcasm_bin = label_binarize(y_test_sarcasm, classes=[0, 1, 2, 3])\n",
    "\n",
    "y_pred_offensive_probs = model.predict([X_test, X_test_text, test_masks])[0] \n",
    "y_pred_sentiment_probs = model.predict([X_test, X_test_text, test_masks])[1] \n",
    "y_pred_sarcasm_probs = model.predict([X_test, X_test_text, test_masks])[2] \n",
    "\n",
    "offensive_classes = ['not_offensive', 'slight', 'very_offensive', 'hateful_offensive']\n",
    "sentiment_classes = ['positive', 'neutral', 'very_positive', 'negative', 'very_negative']\n",
    "sarcasm_classes = ['general', 'twisted_meaning', 'not_sarcastic', 'very_twisted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, class_name in enumerate(offensive_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_offensive_bin[:, i], y_pred_offensive_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{class_name} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.title('Class-Specific ROC Curve - Offensive')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, class_name in enumerate(sentiment_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_sentiment_bin[:, i], y_pred_sentiment_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{class_name} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.title('Class-Specific ROC Curve - Sentiment')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, class_name in enumerate(sarcasm_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_sarcasm_bin[:, i], y_pred_sarcasm_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{class_name} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.title('Class-Specific ROC Curve - Sarcasm')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
