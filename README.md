# Vision-Language-Model
Our model uses a linear self-attentive fusion mechanism to combine Vision Transformer (ViT) features for image analysis and Bidirectional Encoder Representations from Transformers (BERT) for text interpretation. This way, it can see how text and images relate in space and meaning. We test ViT-BERT CAMT on two difficult datasets.
